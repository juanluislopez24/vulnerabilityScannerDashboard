import pandas as pd
import numpy as np

import json

def uploadData(data):
    
    flatten_data = []
    columns = np.array(['ip', 'org', 'hostnames', 'product', 'version', 'longitude', 'latitude', 'os', 'possibleVulns', 'isVulnerable'])

    for match in data:
        
        row = []
        #columns: ip, org, hostnames, product, version, longitude, latitude, os, vulns 
        row.append(match["ip_str"])
        row.append(match["org"])
        row.append(match["hostnames"])
        row.append(match["product"])
        row.append(match.get('version', None))
        row.append(match["location"]["longitude"])
        row.append(match["location"]["latitude"])
        row.append(match["os"])
        try:
            #print([i for i in match['vulns'].keys()])
            row.append([i for i in match['vulns'].keys()])
        except:
            row.append('no vulns')
        row.append(match["Vulnerable"])
        
        
        flatten_data.append(row)
        
    np_data = np.array(flatten_data)
    df = pd.DataFrame(data=np_data,
                    columns=columns)
    
    df.to_gbq('bigData.ipvulns', 
                    'security-papus',
                    chunksize=None, # i've tryed with several chunksizes, it runs faster when is one big chunk (at least for me)
                    if_exists='append',
                    private_key="security-papus-c616ed453af0.json"
                    )
