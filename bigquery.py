import pandas as pd
import numpy as np
from collections import Counter
import json

def uploadData(data, countryCode):
    
    flatten_data = []
    columns = np.array(['ip', 'org', 'hostnames', 'product', 'version', 'geo', 'os', 'possibleVulns', 'isVulnerable', 'countryCode'])
    
    for match in data:
        
        
        row = []
        #columns: ip, org, hostnames, product, version, longitude, latitude, os, vulns 
        row.append(match["ip_str"])
        row.append(match["org"])
        row.append(match["hostnames"])
        row.append(match["product"])
        row.append(match.get('version', None))
        geo = str(match["location"]["latitude"]) + ',' + str(match["location"]["longitude"])
        row.append( geo )
        row.append(match["os"])
        try:
            #print([i for i in match['vulns'].keys()])
            row.append([i for i in match['vulns'].keys()])
        except:
            row.append('no vulns')
        row.append(match["vulnerable"])
        row.append(str(countryCode))
        
        flatten_data.append(row)
           
    np_data = np.array(flatten_data)
    df = pd.DataFrame(data=np_data,
                    columns=columns)
    count = Counter()
    vulns_number = []
    for index, row in df.iterrows():
        vulns_number.append(len(row['possibleVulns']))
        #count.update(row["possibleVulns"])
    df['numberPossibleVulns'] = vulns_number
    
    df.to_gbq('bigData.ipvulns', 
                    'security-papus',
                    chunksize=None, # i've tryed with several chunksizes, it runs faster when is one big chunk (at least for me)
                    if_exists='append',
                    private_key="security-papus-c616ed453af0.json"
                    )
